{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stabti/computer_vision_sessions/blob/main/notebooks/CLIP_Intro_Zero_Shot_Classification_and_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM5P-QjtMVdF"
      },
      "source": [
        "# A Tutorial on CLIP (Contrastive Language-Image Pre-training) for Image-Text Similarity and Zero-Shot Classification\n",
        "\n",
        "#### Author: Antonio Rueda-Toicen\n",
        "**antonio.rueda.toicen 'at' hpi 'dot' de**\n",
        "\n",
        "\n",
        "[![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)\n",
        "\n",
        "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEVGSrTHoCSK"
      },
      "source": [
        "\n",
        "\n",
        "## An Overview of CLIP\n",
        "\n",
        "[CLIP (Contrastive Languageâ€“Image Pre-training)](https://arxiv.org/pdf/2103.00020.pdf) is a model from OpenAI. Given an image and a set of possible text descriptions, the model predicts the most relevant match.\n",
        "\n",
        "CLIP overcomes traditional challenges in computer vision. One is the reliance on model outputs limited to specific tasks. The model learns to put representations of similar concepts next to each other. It does this regardless whether they are coming from text or images. This happens through extensive training with text-image pairs scraped from the internet.\n",
        "\n",
        "CLIP  adapts to new visual classification tasks without changes in the model's architecture. This is zero-shot inference. It differs from standard image classification models. Those only predict the predefined classes that they were trained on.\n",
        "\n",
        "### Contrastive pre-training\n",
        "\n",
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/clip%20contrastive%20pre-training.png?raw=true)\n",
        "\n",
        "CLIP works with two encoders. These are two separate neural networks. One produces embeddings for text and another that produces embeddings for images. Both models â€œspeak the same languageâ€ by encoding similar concepts in text and images. These go into embedding vectors of the same size. The text encoder is a BERT-like transformer model or a Continuous Bag of Words (CBOW) model. The Image Encoder can be either a Vision Transformer or a Resnet.\n",
        "\n",
        "Given $N$ images and their $N$ matching descriptions, CLIP is trained to predict which of the $NXN$ matchings of image pairs occurred.\n",
        "We jointly train an image encoder and a text encoder to maximize cosine similarity of the matching $N$ pairs (main diagonal on the plot), while minimizing\n",
        "similarity of tne $N ^ 2 - N$ incorrect pairings.\n",
        "The cross entropy of images to text and the cross entropy of text to images is added and averaged. This is called 'symmetric cross entropy' and is the 'contrastive loss' used to train the model. The pseudocde of the method, from the original paper by OpenAI, is shown below.\n",
        "\n",
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/Screenshot%202024-03-04%20at%2009.46.49.png?raw=true)\n",
        "\n",
        "\n",
        "With this method, embeddings that represent similar concepts are moved closer together. The effect is what we see in the image below. Embeddings of text and images with high semantic similarity get close to each other. Semantically different text-image pairs are kept further apart.\n",
        "\n",
        "![](https://cdn.sanity.io/images/vr8gru94/production/a54a2f1fa0aeac03748c09df0fdfbb42aadc96b7-2430x1278.png)\n",
        "\n",
        "In this notebook, we use CLIP to extract image embeddings. We don't delve further into contrastive pre-training. In a future session, we will learn how to tune a pretrained CLIP model using LORA (Low Rank Adaptation).\n",
        "\n",
        "### Zero-shot image classification\n",
        "\n",
        "CLIP can be applied to any visual classification benchmark. We do this by providing the names of the visual categories to classify. Zero-shot prediction allows us generalize on unseen labels. We don't need to specifically train the model to classify them. For example, all ImageNet pretrained models recognize 1000 specific classes. We saw this on previous notebooks. CLIP is not bound by this limitation. With CLIP, we can create new labels 'on the fly'. We pass them through the text encoder. Then we use the similarity between text and image embeddings to produce an output.\n",
        "\n",
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/clip%20zero-shot%20prediction.png?raw=true)\n",
        "\n",
        "We explore zero-shot inference with CLIP on this notebook\n",
        "\n",
        "### Limitations and issues\n",
        "\n",
        "CLIP has its limitations. It has difficulty with abstract tasks. It's also difficult to generalize to images outside the pre-training dataset. We sometimes need to finetune the model to do good fine-grained classification.\n",
        "\n",
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/coffee-clip.png?raw=true)\n",
        "\n",
        "Ethical concerns arise due to the potential for biases coming from the dataset.  There are also privacy and copyright issues. A lot of data from the Internet was used to train this model. Despite these challenges, CLIP's ability highlights the usefulness of applying Internet-scale multimodal datasets.\n",
        "\n",
        "The original CLIP model was trained on 400 million image-text pairs. This was done using 256 V100 Nvidia GPUs. This scale is unachievable by most companies and private individuals. It's difficult to create a CLIP model from scratch that performs as well as the pretrained ones by OpenAI or [LAION](https://laion.ai/blog/large-openclip/).\n",
        "\n",
        "In this notebook we will explore both the capabilities and limitations of CLIP. We will ask it to produce image embeddings and classifications for paintings. We will explore the Tower of Babel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPWmQKpKR03F"
      },
      "source": [
        "## The Tower of Babel by Peter Bruegel the Elder (and others)\n",
        "\n",
        "\n",
        "Pieter Bruegel the Elder created three paintings on the theme of the [Tower of Babel](https://en.wikipedia.org/wiki/The_Tower_of_Babel_%28Bruegel%29), with two surviving versions housed in Vienna's Kunsthistorisches Museum and Rotterdam's Museum Boijmans Van Beuningen. These paintings depict the biblical story of humanity building a tower to reach the heavens, as described in Genesis. In the story, God is displeased with this development and creates languages (according to Judeochristian tradition there was a single one before this event) in order to sabotage the project and punish humans for their hubris.\n",
        "\n",
        "A notable feature of Bruegel's paintings is the architectural similarity of the tower to the [Roman Colosseum](https://en.wikipedia.org/wiki/The_Tower_of_Babel_%28Bruegel%29#Architecture), symbolizing hubris and persecution. This thematic choice reflects the religious tensions of Bruegel's time.\n",
        "\n",
        "The theme of the Tower of Babel was popular among other artists as well. Lucas van Valckenborch and Pieter Bruegel the Younger (the Elder's son) also created their own interpretations of the Tower of Babel, further emphasizing its impact on art history.\n",
        "\n",
        "We use these paintings and their descriptions to explore CLIP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbPUNpgaCUfT"
      },
      "outputs": [],
      "source": [
        "import skimage.io as io\n",
        "from PIL import Image\n",
        "from collections import OrderedDict\n",
        "\n",
        "url_a = 'https://artinwords.de/wp-content/uploads/Pieter-Bruegel-Turmbau-zu-Babel.jpg'\n",
        "description_a = 'The (Great) Tower of Babel by Peter Bruegel the Elder'\n",
        "img_a = Image.fromarray(io.imread(url_a))\n",
        "img_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rck_OR7TG2F8"
      },
      "outputs": [],
      "source": [
        "url_b = 'https://www.bruegel2018.at/fileadmin/user_upload/Cat_65-HR-Tower-of-Babel.jpg'\n",
        "description_b =  'The (Little) Tower of Babel by Peter Bruegel the Elder'\n",
        "img_b = Image.fromarray(io.imread(url_b))\n",
        "img_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMOfe2unH49C"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url_c = 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/La_Tour_de_Babel%2C_Van_Valckenborch%2C_1594.jpg/1200px-La_Tour_de_Babel%2C_Van_Valckenborch%2C_1594.jpg'\n",
        "description_c =  \"Van Valckenborch's Tower of Babel\"\n",
        "# Use requests to get the image content with a User-Agent header\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "response = requests.get(url_c, headers=headers, stream=True)\n",
        "response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "img_c = Image.open(response.raw)\n",
        "img_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9HJNvsGIYN4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "url_d = 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Pieter_Bruegel_d._%C3%84._109.jpg/1600px-Pieter_Bruegel_d._%C3%84._109.jpg'\n",
        "description_d = \"King and Entourage - Detail from the Great Tower of Babel\"\n",
        "# Use requests to get the image content with a User-Agent header\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "response = requests.get(url_d, headers=headers, stream=True)\n",
        "response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "img_d = Image.open(response.raw)\n",
        "img_d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUzBPosaJo2g"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "url_e = 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/The_Tower_of_Babel%2C_oil_on_panel_painting_by_Pieter_Brueghel_the_Younger.jpg/1476px-The_Tower_of_Babel%2C_oil_on_panel_painting_by_Pieter_Brueghel_the_Younger.jpg?20161130233735'\n",
        "description_e =  \"Tower of Babel by Peter Brugel the Younger\"\n",
        "# Use requests to get the image content with a User-Agent header\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "response = requests.get(url_e, headers=headers, stream=True)\n",
        "response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "img_e = Image.open(response.raw)\n",
        "img_e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YS8qV1YYpX9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "url_f = 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Hieronymus_Cock_-_Kolosseum_1551_beschnitten.jpg/1600px-Hieronymus_Cock_-_Kolosseum_1551_beschnitten.jpg'\n",
        "description_f = \"A sketch of the Roman Colosseum by Hieronymus Cock\"\n",
        "# Use requests to get the image content with a User-Agent header\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "response = requests.get(url_f, headers=headers, stream=True)\n",
        "response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "img_f = Image.open(response.raw)\n",
        "img_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hh3dvbigZhl"
      },
      "outputs": [],
      "source": [
        "url_g = 'https://github.com/andandandand/images-for-colab-notebooks/blob/main/babellora02.png?raw=true'\n",
        "description_g = \"Bruegel's Tower of Babel, generated by Stable Diffusion XL 1.0\"\n",
        "img_g = Image.fromarray(io.imread(url_g))\n",
        "img_g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq2ORENoaPTX"
      },
      "source": [
        "## Importing CLIP from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMl_7rD6CmeD"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPModel, CLIPProcessor\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr6IR4_kCzu6"
      },
      "outputs": [],
      "source": [
        "# We can inspect the model's architecture, which is PyTorch code.\n",
        "# We have downloaded both the architecture and the model's weights.\n",
        "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azerd-PBD4z0"
      },
      "source": [
        "The `transformers` library from Huggingface allows us to use many pre-trained models.\n",
        "\n",
        "`CLIPModel.from_pretrained('openai/clip-vit-base-patch32')` is a function call that loads a pre-trained CLIP model, the `clip-vit-base-patch32` version.\n",
        "\n",
        "Here's what happens during this function call:\n",
        "\n",
        "1. Model Architecture Loading: The `CLIPModel` class represents the CLIP model architecture. When we call `from_pretrained()`, it initializes a model with the architecture defined for CLIP.\n",
        "\n",
        "2. Pre-trained Weights: The string `'openai/clip-vit-base-patch32'`  is a set of weights. These weights are in the Hugging Face model hub. OpenAI produced these with the CLIP learning method on a large dataset of images and their descriptions.\n",
        "\n",
        "3. Vision Transformer Variant: `vit-base-patch32` indicates that the model uses the [Vision Transformer (ViT)](https://paperswithcode.com/method/vision-transformer) architecture. The input images are divided into patches of size 32x32 pixels before processing by the transformer.\n",
        "We could also choose a [ResNet (convolutional neural network)](https://paperswithcode.com/method/resnet) instead.\n",
        "\n",
        "4. Downloading and Caching: If this is the first time we're using this model in the running working space, the weights are downloaded from the Hugging Face model hub and cached locally. The following usage of `from_pretrained()` will use the local cache, without downloading the weights again.\n",
        "\n",
        "5. Instantiation and Readiness for Inference: After the model weights load, the CLIP model is ready for inference. You can then process images and text to extract text of image features (vector embeddings) or perform zero-shot classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mhPG-99CvjO"
      },
      "outputs": [],
      "source": [
        "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "processor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkTDbvHgDXGS"
      },
      "source": [
        "The `CLIPProcessor` prepares data for CLIP. The model requires both images and text to be in a specific format before being fed to the encoding networks. Here's how it works:\n",
        "\n",
        "1. **Resizing and Normalization**: The processor takes an image input. It then resizes it to the dimensions expected by the model (e.g., 224x224 pixels). It then normalizes the image by scaling pixel values to a range that the model was trained on, typically [0, 1] or [-1, 1]. It then  aligns it with the color channel means and standard deviations that the model expects. For this model these are the mean and standard deviations fof the RGB channels on the [CLIP training dataset](https://arxiv.org/pdf/2103.00020.pdf). CLIP was trained from scratch by OpenAI without using ImageNet weights for the visual encoder or other weights for the text encoder.\n",
        "\n",
        "2. **Tokenization**: For the text inputs, the processor [tokenizes the sentences](https://claritynlp.readthedocs.io/en/latest/developer_guide/algorithms/sentence_tokenization.html). We convert the text to tokens (often words or syllables) that are represented by numerical IDs. These IDs correspond to entries in the model's vocabulary.\n",
        "\n",
        "3. **Padding and Attention Mask**: The processor pads the token sequences. They are made to be the same length for batch processing. It also creates attention masks that allow the model to ignore padding tokens during processing.\n",
        "\n",
        "4. **Conversion to PyTorch Tensors**: The processor converts the processed image and text data into PyTorch tensors. Tensors are multi-dimensional arrays suitable for input into the model.\n",
        "\n",
        "5. **Return Tensors**: The processed tensors return in a format that can be fed into the CLIP model. Now we can produce classifications or embeddings from them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRE1ayi7bkSf"
      },
      "source": [
        "## Producing Text Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng8yU3dbfdW1"
      },
      "outputs": [],
      "source": [
        "# We create a list of descriptions to pass to the processor\n",
        "descriptions = [description_a, description_b, description_c, description_d,\n",
        "                description_e, description_f, description_g]\n",
        "descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XffquCcodjNr"
      },
      "outputs": [],
      "source": [
        "inputs = processor(text=descriptions, return_tensors=\"pt\",\n",
        "                   padding=True, truncation=True)\n",
        "\n",
        "\n",
        "# We use torch.no_grad() to avoid having to call .detach() on the tensor\n",
        "with torch.no_grad():\n",
        "    text_embeddings = model.get_text_features(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gQ5WlvBdxA8"
      },
      "outputs": [],
      "source": [
        "text_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnY3uX_EDKUP"
      },
      "outputs": [],
      "source": [
        "inputs = processor(images=[img_a, img_b, img_c, img_d, img_e, img_f, img_g],\n",
        "                   return_tensors=\"pt\")\n",
        "inputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfkC-5DyEsUb"
      },
      "source": [
        "The line `inputs = processor(images=img_a, return_tensors=\"pt\")` is using the `CLIPProcessor` to preprocess an image so that it can be inputted into the CLIP model for inference. Here is a breakdown of what each part of this line of code does:\n",
        "\n",
        "1. **processor(images=img_a)**: This part of the code calls the `processor` with the image `img_a` as an argument. The `images` parameter is where we pass the image or images we want to process. The processor will handle the necessary transformations of this image so that it is in the correct format for the CLIP model.\n",
        "\n",
        "2. **return_tensors=\"pt\"**: This argument tells the processor that we want the output to be PyTorch tensors. The `\"pt\"` stands for PyTorch. If we were working with TensorFlow, for example, we might use `\"tf\"` to get TensorFlow tensors instead.\n",
        "\n",
        "3. **inputs**: This variable is being assigned the output of the processor. This output will be a dictionary containing everything the model needs to run its predictions. This typically includes the processed pixel values of the image, now as a tensor, and an attention mask indicating which parts of the tensor are actual data and which parts are padding.\n",
        "\n",
        "The processor's output, which is now stored in `inputs`, can be directly fed into the CLIP model to obtain embeddings or perform inference. It simplifies the process by abstracting the preprocessing steps such as resizing, normalization, and conversion to tensors, which are all necessary to prepare the image data for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk1oAVU_b2JI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.array(img_a).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlETUz8cHRC2"
      },
      "outputs": [],
      "source": [
        "index=0\n",
        "inputs['pixel_values'][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb0zqUE-U7KG"
      },
      "outputs": [],
      "source": [
        "np.array(img_a).min(), np.array(img_a).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQz1l06yLUJG"
      },
      "outputs": [],
      "source": [
        "inputs['pixel_values'][0].min(), inputs['pixel_values'][0].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLWREtxTVrTL"
      },
      "source": [
        "## Effects of Image Resizing and Normalization\n",
        "\n",
        "The `CLIPProcessor` from the Hugging Face Transformers library preprocesses images in a specific way for use with CLIP models. The observed range of values outside of -1 and 1 after processing with `CLIPProcessor` might be unexpected when considering the usual normalization technique, which aims to scale pixel values between -1 and 1 based on the mean and standard deviation of the data. However, the range can exceed these bounds due to the specifics of how we are doing preprocessing.\n",
        "\n",
        "When an image is processed by `CLIPProcessor`, several steps are typically applied:\n",
        "\n",
        "1. **Resizing and Cropping:** The image is resized and cropped to match the input size expected by the CLIP model. In our example, we turn our image of arbitrary size and aspect ratio into a square tensor of size (224, 224).\n",
        "\n",
        "2. **Normalization:** The pixel values, which are initially in the range [0, 255], are normalized. The normalization is done using pre-defined mean and standard deviation values. The formula used is:\n",
        "\n",
        "   $$\n",
        "   \\text{normalized value} = \\frac{\\text{value} - \\text{mean from training set}}{\\text{std from training set}}\n",
        "  $$\n",
        "  \n",
        "   For CLIP models, the mean and standard deviation are usually set to values that are specific to the dataset on which CLIP was trained. Each R,G,B channel from the training set has its own mean and standard deviation.\n",
        "\n",
        "The range of normalized values exceeding -1 and 1 can occur if the original pixel values of the image significantly deviate from the expected mean, even after normalization. This deviation can result from the specific mean and standard deviation values used in the normalization step, which come from the distribution of the dataset on which CLIP was trained.\n",
        "\n",
        "The normalization step assumes that the input pixel values are distributed in a certain way, usually centered around the dataset mean. If an image has pixel values that are not well represented by the dataset mean and standard deviation (for example, if the image is significantly brighter or darker than the average image in the dataset), the normalized values can fall outside the expected range of -1 to 1.\n",
        "\n",
        "In summary, the observed range of values outside of -1 and 1 after preprocessing with `CLIPProcessor` is a result of how the normalization step interacts with the specific characteristics of your input images relative to the expected distribution of the dataset used to train the CLIP model. This behavior is typically not an issue for the model, as CLIP and similar neural networks are generally robust to variations in input ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "049gwDl-akGE"
      },
      "outputs": [],
      "source": [
        "# @title How the model 'views' the data {run:'auto'}\n",
        "slider_value = 3  # @param {type: \"slider\", min: 0, max: 5}\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(inputs['pixel_values'][slider_value].permute(1,2,0))#.view(224, 224, 3));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhXcqZ9gawoO"
      },
      "source": [
        "## Producing image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzsQM7ekEV14"
      },
      "outputs": [],
      "source": [
        "image_embeddings = model.get_image_features(**inputs).detach()\n",
        "image_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2XiOIumJPAt"
      },
      "outputs": [],
      "source": [
        "image_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ0IQW5fFe4A"
      },
      "source": [
        "The line `outputs = model.get_image_features(**inputs)` is invoking a method of the `CLIPModel` instance to obtain image features from the preprocessed inputs.\n",
        "This line is where the actual computation happens to transform your image into a fixed-size vector (embedding) that captures the visual essence of the image, as learned by the model during its training on a diverse set of images and text descriptions.\n",
        "\n",
        "Here's a step-by-step explanation:\n",
        "\n",
        "- **model**: This is the instance of the `CLIPModel` that we have loaded using `CLIPModel.from_pretrained('openai/clip-vit-base-patch32')`. It encapsulates the pre-trained CLIP model.\n",
        "\n",
        "- **get_image_features**: This method of the `CLIPModel` class is used to get the image features from the input data. Image features are essentially a vector of numbers that represent the contents of the image in a way that can be understood and used by machine learning models.\n",
        "\n",
        "- **(**inputs**)**: The double asterisk `**` is used to unpack the `inputs` dictionary into keyword arguments. This means that if `inputs` contains, `{'pixel_values':tensor, 'attention_mask': tensor}`, calling `**inputs` would be like passing `pixel_values=tensor, attention_mask=tensor` directly to the function.\n",
        "\n",
        "- **outputs**: The variable `outputs` is being assigned the result of `get_image_features`. After this line executes, `outputs` will contain the image embeddings, which are the feature representations of the input image. These embeddings capture the visual information in a form that can be used for comparison with other images, classification, and other tasks that the CLIP model is capable of.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZded9i6Qhkj"
      },
      "outputs": [],
      "source": [
        "image_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3sDwS37MNuv"
      },
      "outputs": [],
      "source": [
        "image_embeddings[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8CTm2qFoXol"
      },
      "source": [
        "## `torch.nn.CosineSimilarity`\n",
        "\n",
        "\n",
        "\n",
        "* `dim`: This parameter specifies the dimension along which cosine similarity is computed. `dim=0` means that the similarity will be computed along the first dimension (i.e., the rows if we think of a 2D tensor as a matrix).\n",
        "\n",
        "* `eps`: This is a small value added to the denominator for numerical stability. In the code eps=1e-6, it prevents division by zero when normalizing vectors. This is especially useful when dealing with very small values in the vectors.\n",
        "\n",
        "$$\n",
        "\\text{cosine similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUw-m1JUMLEt"
      },
      "outputs": [],
      "source": [
        "cosine_similarity = torch.nn.CosineSimilarity(dim=0, eps=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gjm-qJXsIw4"
      },
      "outputs": [],
      "source": [
        "def show_pair(imag_a, imag_b):\n",
        "  plt.subplot(121)\n",
        "  plt.imshow(np.array(imag_a))\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.imshow(np.array(imag_b))\n",
        "  plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIjf_HVbsXfW"
      },
      "outputs": [],
      "source": [
        "# We create two lists to go through the images and descriptions\n",
        "images = [img_a, img_b, img_c, img_d, img_e, img_f, img_g]\n",
        "descriptions = [description_a, description_b, description_c, description_d,\n",
        "                description_e, description_f, description_g]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe431KkvpPGR"
      },
      "source": [
        "## Evaluating the similarity of image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3XsD-tzvUya"
      },
      "outputs": [],
      "source": [
        "# @title {run:'auto'}\n",
        "\n",
        "slider_value_1 = 3  # @param {type: \"slider\", min: 0, max: 7}\n",
        "slider_value_2 = 1  # @param {type: \"slider\", min: 0, max: 7}\n",
        "\n",
        "\n",
        "print(f'Cosine similarity = {cosine_similarity(image_embeddings[slider_value_1], image_embeddings[slider_value_2]):.2f}')\n",
        "show_pair(images[slider_value_1], images[slider_value_2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDfcAI47QM4b"
      },
      "source": [
        "## Evaluating the similarity of text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKj3caPFALgy"
      },
      "outputs": [],
      "source": [
        "# @title {run:'auto'}\n",
        "\n",
        "slider_value_1 = 0  # @param {type: \"slider\", min: 0, max: 5}\n",
        "slider_value_2 = 1  # @param {type: \"slider\", min: 0, max: 5}\n",
        "\n",
        "\n",
        "print(f\"\"\"Cosine similarity = {cosine_similarity(text_embeddings[slider_value_1],\n",
        "                                               text_embeddings[slider_value_2]):.2f}\"\"\")\n",
        "print(f\"\"\"First description: {descriptions[slider_value_1]}\\nSecond description: {descriptions[slider_value_2]}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebxdk-OEQRYJ"
      },
      "source": [
        "## Evaluating the text-image similarity of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TBsyz_LBY-i"
      },
      "outputs": [],
      "source": [
        "# @title {run:'auto'}\n",
        "\n",
        "slider_value_1 = 0  # @param {type: \"slider\", min: 0, max: 5}\n",
        "slider_value_2 = 0  # @param {type: \"slider\", min: 0, max: 5}\n",
        "\n",
        "\n",
        "print(f\"\"\"Cosine similarity = {cosine_similarity(text_embeddings[slider_value_1],\n",
        "                                                image_embeddings[slider_value_2]):.2f}\"\"\")\n",
        "print(f\"\"\"Description: {descriptions[slider_value_1]}\"\"\")\n",
        "images[slider_value_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k8WEJmZDN2n"
      },
      "outputs": [],
      "source": [
        "text_embeddings.shape, image_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuDvCzUtDv02"
      },
      "outputs": [],
      "source": [
        "# Notice that the embeddings are not normalized\n",
        "text_embeddings.max(), image_embeddings.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzUCYaCmEdIa"
      },
      "outputs": [],
      "source": [
        "# We normalize embeddings (the cosine_similarity function did this for us before)\n",
        "image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
        "text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
        "text_embeddings.max(), image_embeddings.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgbwbI1AGGpm"
      },
      "outputs": [],
      "source": [
        "similarity = text_embeddings @ image_embeddings.T\n",
        "similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0fK5S3AFFDS"
      },
      "outputs": [],
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "plt.yticks(range(count), descriptions, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image embeddings\", size=20);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j04lw8p1pX1E"
      },
      "source": [
        "## Running CLIP as a zero-shot classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmP4S6n5XNkM"
      },
      "outputs": [],
      "source": [
        "\n",
        "text = ['a photograph',\n",
        "        'a painting',\n",
        "        'a cartoon',\n",
        "        'a comic',\n",
        "        'a newspaper',\n",
        "        'live people standing in front of a painting',\n",
        "        'a painting by Peter Bruegel the elder',\n",
        "        'a painting by Peter Bruegel the younger',\n",
        "        'a painting by Picasso',\n",
        "        'a painting by Dali',\n",
        "        'a painting by PJ Crook',\n",
        "        'a painting by Van Valckenborch',\n",
        "        'a sketch of the Roman Colosseum',\n",
        "        'The Great Tower of Babel by Peter Bruegel the Elder',\n",
        "        'The  Tower of Babel by Peter Bruegel the Younger',\n",
        "        'The Little Tower of Babel by Peter Bruegel the Elder',\n",
        "        'The Tower of Babel by Van Valckenborch',\n",
        "        'The Tower of Babel generated by Stable Diffusion'\n",
        "        ]\n",
        "\n",
        "inputs = processor(text=text,\n",
        "                   images=[img_a, img_b, img_c, img_d, img_e, img_f],\n",
        "                   return_tensors=\"pt\", padding=True)\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image\n",
        "temperature = 1.0\n",
        "outputs.logits_per_image.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spTa5iXJLppl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4THiiSRNjoXt"
      },
      "source": [
        "### Understanding softmax's temperature parameter\n",
        "\n",
        "In machine learning, especially in the context of neural networks, we often use the softmax function for multi-class classification problems. It transforms a vector of raw scores (logits) from the model into probabilities. This is achieved by taking the exponential of each element and then normalizing these values by dividing by the sum of all these exponentials. This process ensures that the output values fall within the range (0, 1) and sum up to 1, making them interpretable as probabilities.\n",
        "\n",
        "The temperature parameter $(T$) of the softmax function is a hyperparameter that influences the \"sharpness\" of the output probability distribution. The modified softmax function with temperature is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(\\mathbf{z_i}) = \\frac{e^{\\frac{z_i}{T}}}{\\sum_{j} e^{\\frac{z_j}{T}}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\mathbf{z}$ is the input vector containing the raw class scores from the model,\n",
        "- $z_i$ is the score for class $i$,\n",
        "- $T$ is the temperature parameter, and\n",
        "- $i$ ranges over all possible classes.\n",
        "\n",
        "### Impact of Temperature $T$\n",
        "\n",
        "- **$T = 1$:** This represents the standard softmax function without modification, keeping the differences in the scores unchanged.\n",
        "\n",
        "- **$T > 1$:** Increasing the temperature makes the softmax function \"softer\", leading to a more uniform probability distribution. Higher temperatures result in smaller differences between the largest and smallest probabilities, promoting exploration in models used in reinforcement learning or in sequence generation where output diversity is desired.\n",
        "\n",
        "- **$T < 1$:** Decreasing the temperature makes the softmax function \"sharper\", amplifying the differences between the scores. A lower temperature results in a greater disparity between the highest probability and the rest, enhancing the model's confidence (though potentially increasing the risk of misplaced confidence). This sharper distribution can be beneficial in scenarios requiring more decisive actions.\n",
        "\n",
        "### Example\n",
        "\n",
        "Let's consider a vector of logits $\\mathbf{z} = [2, 1, 0.1]$ and apply the softmax function with different temperatures:\n",
        "\n",
        "- **For $T = 1$:**\n",
        "  - The differences between the scores are maintained as originally presented.\n",
        "- **For $T > 1$, say $T = 2$:**\n",
        "  - The softmax output makes the scores closer to each other, leading to a more uniform probability distribution.\n",
        "- **For $T < 1$, say $T = 0.5$:**\n",
        "  - The highest score is significantly more emphasized in the probability distribution, making it peakier.\n",
        "\n",
        "By adjusting the temperature parameter, we can control the level of confidence in the predictions our model makes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi7YpUo6yiKh"
      },
      "outputs": [],
      "source": [
        "# @title Probabilities at different temperatures {run:'auto'}\n",
        "\n",
        "slider_value = 0  # @param {type: \"slider\", min: 0, max: 5}\n",
        "temperature = 0.7  # @param {type: \"slider\", min: 0, max: 2, step:0.1}\n",
        "\n",
        "# Creating a subplot with an image in the first row and the histogram in the second row\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "axes[0].imshow(np.array(images[slider_value]))\n",
        "axes[0].axis('off')  # Turning off the axis for the image\n",
        "axes[0].set_title(descriptions[slider_value])\n",
        "\n",
        "probs = (logits_per_image/temperature).softmax(dim=1)\n",
        "\n",
        "# Creating the horizontal bar plot in the second subplot\n",
        "axes[1].barh(text, probs[slider_value].detach(), color='skyblue')\n",
        "axes[1].set_xlim(0, 1)  # Setting the x-axis limit from 0 to 1\n",
        "\n",
        "axes[1].set_xlabel('Probabilities')\n",
        "axes[1].set_title('Probabilities of Labels')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZSRZQXmLrCG"
      },
      "source": [
        "## Now try it on a cat image! ðŸ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEhcKg4uMU5Y"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "cats_img = Image.open(requests.get(url, stream=True).raw)\n",
        "cats_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SSNujV9Lx2W"
      },
      "outputs": [],
      "source": [
        "# Notice what happens with the output probabilities\n",
        "# when we make the labels more specific\n",
        "cat_text = ['a cat',\n",
        "            'a dog',\n",
        "           #'two cats',\n",
        "          # 'two cats lying on a sofa',\n",
        "          # 'two cats lying on a sofa next to two tv controllers'\n",
        "           ]\n",
        "\n",
        "inputs = processor(text=cat_text,\n",
        "                   images=[cats_img],\n",
        "                   return_tensors=\"pt\", padding=True)\n",
        "cat_outputs = model(**inputs)\n",
        "cat_logits_per_image = cat_outputs.logits_per_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j-1gKYcM1Od"
      },
      "outputs": [],
      "source": [
        "# Probabilities at different temperatures @title {run:'auto'}\n",
        "\n",
        "temperature = 1.71  # @param {type: \"slider\", min: 0.01, max: 2, step:0.1}\n",
        "\n",
        "# Creating a subplot with an image in the first row and the histogram in the second row\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "axes[0].imshow(np.array(cats_img))\n",
        "axes[0].axis('off')  # Turning off the axis for the image\n",
        "\n",
        "cat_probs = (cat_logits_per_image/temperature).softmax(dim=1)\n",
        "\n",
        "# Creating the horizontal bar plot in the second subplot\n",
        "axes[1].barh(cat_text, cat_probs.detach().numpy().flatten(), color='skyblue')\n",
        "axes[1].set_xlim(0, 1)  # Setting the x-axis limit from 0 to 1\n",
        "\n",
        "axes[1].set_xlabel('Probabilities')\n",
        "axes[1].set_title('Probabilities of Different Labels')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZma3abIPbuU"
      },
      "source": [
        "## What about the lion-boar? ðŸ¦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5fcbUD8Ppoy"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://github.com/andandandand/images-for-colab-notebooks/blob/main/Screenshot%202024-02-13%20at%2011.44.27.png?raw=true\"\n",
        "lion_boar_img = Image.open(requests.get(url, stream=True).raw)\n",
        "lion_boar_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yGEDbHJP-zQ"
      },
      "outputs": [],
      "source": [
        "lion_boar_text = ['a lion',\n",
        "                  'a boar',\n",
        "                  'a female lion',\n",
        "                  'a male lion',\n",
        "                  'a lion cub'\n",
        "                ]\n",
        "\n",
        "inputs = processor(text=lion_boar_text,\n",
        "                   images=[lion_boar_img],\n",
        "                   return_tensors=\"pt\", padding=True)\n",
        "lion_boar_outputs = model(**inputs)\n",
        "lion_boar_logits_per_image = lion_boar_outputs.logits_per_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2dsOPGDQwDb"
      },
      "outputs": [],
      "source": [
        "# Probabilities at different temperatures @title {run:'auto'}\n",
        "\n",
        "temperature = 1.71  # @param {type: \"slider\", min: 0.01, max: 2, step:0.1}\n",
        "\n",
        "# Creating a subplot with an image in the first row and the histogram in the second row\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "axes[0].imshow(np.array(lion_boar_img))\n",
        "axes[0].axis('off')  # Turning off the axis for the image\n",
        "\n",
        "lion_boar_probs = (lion_boar_logits_per_image/temperature).softmax(dim=1)\n",
        "\n",
        "# Creating the horizontal bar plot in the second subplot\n",
        "axes[1].barh(lion_boar_text, lion_boar_probs.detach().numpy().flatten(), color='skyblue')\n",
        "axes[1].set_xlim(0, 1)  # Setting the x-axis limit from 0 to 1\n",
        "\n",
        "axes[1].set_xlabel('Probabilities')\n",
        "axes[1].set_title('Probabilities of Different Labels')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf4CWa8wbldx"
      },
      "source": [
        "Please continue the exploration of class attribution [in this notebook](https://colab.research.google.com/drive/1oItQTATutrRrFSvXv2ThPdr4vy1ogRWN?usp=sharing) (Requires GPU usage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjmIeA49FBBZ"
      },
      "source": [
        "## References\n",
        "\n",
        "* [OpenAI's CLIP Announcement](https://openai.com/research/clip)\n",
        "\n",
        "* [Learning Transferable Visual Models From Natural Language Supervision\n",
        "](https://arxiv.org/abs/2103.00020)\n",
        "\n",
        "* [Pinecone's CLIP Tutorial](https://www.pinecone.io/learn/series/image-search/clip/)\n",
        "\n",
        "* [Wikipedia's article on Peter Bruegel's Tower of Babel](https://en.wikipedia.org/wiki/The_Tower_of_Babel_%28Bruegel%29)\n",
        "\n",
        "* [List of Paintings by Peter Bruegel the Elder](https://en.wikipedia.org/wiki/List_of_paintings_by_Pieter_Bruegel_the_Elder)\n",
        "\n",
        "* [CLIP on HuggingFace](https://huggingface.co/docs/transformers/model_doc/clip)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}